\documentclass[uplatex, 11pt, a4j, dvipdfmx]{jsarticle}

%------------------------------------------------------------------------
% パッケージ
%------------------------------------------------------------------------
\usepackage{./preambles/ownpack}


\title{誤差逆伝播法(Backpropagation)}
\author{}
\date{}

% -----------------------------------------------------------------------
% 本文
% -----------------------------------------------------------------------

\begin{document}

  \maketitle
  \tableofcontents

  \section{概要(abstract)}
    誤差逆伝播法(Backpropagation)はフィードフォワード型のニューラルネットワークの学習に用いられる方法である。
    確率的勾配法(stochastic gradient descent)と呼ばれる方法の一つであり、広い意味では最小値を探索するアルゴリズムである。
    ニューラルネットワークの学習は、損失関数をパラメータの関数と考えたときに、損失関数を目的関数とした最小化問題であるということができる。
    最小化問題としてのニューラルネットワークに対して確率的勾配法を適用したのが誤差逆伝播法ということができる。

  \section{アルゴリズム(algorithm)}
    まず使用するニューラルネットワークを定義する。
    対象とするニューラルネットワークの層数を$N$とする。
    $k \; (k = 1, 2, \cdots, N)$層目のニューロン$k \; (i = 1, 2, \cdots, n_k)$の出力値を$x^{(k)}_i$、$k+1$層目のニューロンの計算に用いられるパラメータを$w^{(k)}_i \; i = 1, 2, \cdots, m_k$とする。
    ここで$n_k$は$k$層目のニューロン数、$m_k$は$k+1$層目のニューロンの出力値の計算に用いられるパラメータの数である。
    また、$k+1$層目のニューロン$i$の出力値を計算する関数を$f^{(k)}_i$とする。
    これらによって順伝播の計算は
    \begin{equation}
      x^{(k+1)}_i = \func{f^{(k)}_i}{x^{(k)}, w^{(k)}}
    \end{equation}
    として書かれる。
    また、損失関数を$L$とすると、$p$個目の教師パターン$t^{(p)}_i \; (i = 1, 2, \cdots, n_{N})$とネットワークの出力$x^{(N)}_i$との誤差は
    \begin{equation}
      \func{L}{t^{(p)}, x^{(N)}}
    \end{equation}
    となる。
    よって教師パターン$t^{(p)}$に対する各重みの更新量$\Delta w^{(k)}_i$は
    \begin{equation}
      \Delta w^{(k)}_i = -\eta \diffp{L}{w^{(k)}_i}
    \end{equation}
    として書かれる。
    更新量は連鎖律によって
    \begin{equation}
      \Delta w^{(k)}_i = -\eta \partial^{(p)} x^{(k)}_j \partial^{(p)} w^{(k)}_i
    \end{equation}
    と変形できる。
    ここで
    \begin{align}
      \partial^{(p)} x^{(k)}_i &= \begin{cases}
        \diffp{L}{x^{(N)}_i} = \partial_{x_i} \func{L}{t^{(p)}, x^{(N)}}, & k = N \\
        \sum_j \diffp{L}{x^{(k+1)}_j} \diffp{x^{(k+1)}_j}{x^{(k)}_i} = \sum_j \partial^{(p)} x^{(k+1)}_j \cdot \partial_{x_i} \func{f^{(k)}_j}{x^{(k)}, w^{(k)}}, & \text{otherwise}
      \end{cases} \\
      \partial^{(p)} w^{(k)}_i &= \sum_j \partial^{(p)} x^{(k+1)}_j \cdot \partial_{w_i} \func{f^{(k)}_j}{x^{(k)}, w^{(k)}} \\
      \partial_{x_i} f^{(k)}_j &= \diffp{f^{(k)}_j}{x^{(k)}_i} = \diffp{x^{(k+1)}_j}{x^{(k)}_i} \\
      \partial_{w_i} f^{(k)}_j &= \diffp{f^{(k)}_j}{w^{(k)}_i} = \diffp{x^{(k+1)}_j}{w^{(k)}_i}
    \end{align}
    である。

    誤差逆伝播法の具体的な手順は以下の通りである。
    \begin{description}
      \descitem{Step1}{itm:BP:algorithm:step1}
        教師パターンの組$(x'^{(p)}, t^{(p)})$を$P$個用意する。
        学習率$\eta$を定める。

      \descitem{Step2}{itm:BP:algorithm:step2}
        ある$p$に対して
        \begin{equation}
          x^{(1)}_i = x'^{(p)}_i
        \end{equation}
        とし、$k$を$2$から$N$まで増やしながら
        \begin{equation}
          x^{(k)}_i = \func{f^{(k-1)}_i}{x^{(k-1)}, w^{(k-1)}}
        \end{equation}
        を計算する。

      \descitem{Step3}{itm:BP:algorithm:step3}
        ネットワークの出力と教師パターンとの誤差から
        \begin{equation}
          \partial^{(p)} x^{(N)}_i = \partial_{x_i} \func{L}{t^{(p)}, x^{(N)}}
        \end{equation}
        を計算し、$k$を$N-1$から$1$まで減らしながら
        \begin{align}
          \partial^{(p)} x^{(k)}_i = \sum_j \partial^{(p)} x^{(k+1)}_j \cdot \partial_{x_i} \func{f^{(k)}_j}{x^{(k)}, w^{(k)}} \\
          \partial^{(p)} w^{(k)}_i = \sum_j \partial^{(p)} x^{(k+1)}_j \cdot \partial_{w_i} \func{f^{(k)}_j}{x^{(k)}, w^{(k)}}
        \end{align}
        を計算する。

      \descitem{Step4}{itm:BP:algorithm:step4}
        いくつかの$p$に対して\ref{itm:BP:algorithm:step2}から\ref{itm:BP:algorithm:step3}を繰り返し行い、重みの更新量
        \begin{equation}
          \Delta w^{(k)}_i = -\eta \sum_p \partial^{(p)} w^{(k)}_i
        \end{equation}
        を計算し重みを更新する。

      \descitem{Step5}{itm:BP:algorithm:step5}
        繰り返し回数の上限を越えた場合やネットワークの出力と教師パターンの誤差
        \begin{equation}
          E = \sum_p \func{L}{t^{(p)}, x^{(N)}}
        \end{equation}
        が一定の値以下になった場合に終了する。
        そうでない場合\ref{itm:BP:algorithm:step2}へ戻る。

    \end{description}


  \section{全結合層(fully-connected layer)}
    ニューロン数$n$の層からニューロン数$m$の層への全結合層の順伝播は
    \begin{equation}
      y_j = \func{f}{x, w} = \sum_i x_j w_{ij}
    \end{equation}
    と書くことができる。
    ここで$i$は前の層のニューロンの番号、$j$は次の層のニューロンの番号、$w_{ij}$は$i$番目のニューロンから$j$番目のニューロンへの結合荷重である。
    前節の式とてらしあわせると
    \begin{align}
      \partial_{x_i} \func{f_j}{x, w} = \diffp*{\sum_k x_k w_{kj}}{x_i} = \begin{cases}
        w_{ij}, & i = k \\
        0, & \text{otherwise}
      \end{cases} \\
      \partial_{w_{ik}} \func{f_j}{x, w} = \diffp*{\sum_l x_l w_{lj}}{w_{ik}} = \begin{cases}
        x_i, & i = l, j = k \\
        0, & \text{otherwise}
      \end{cases}
    \end{align}
    となる。


  \section{畳み込み層(convolution layer)}
    前層と後層のニューロンが$n$次元で構成され、各次元$i$でのサイズが$\mu_i$であるとする。
    このとき各次元$i$に対してストライドを$s_i$、パディングのサイズを$p_i$、フィルタのサイズを$f_i$とすると、出力される信号長$\nu_i$は
    \begin{equation}
      \nu_i = \frac{\mu_i + 2 p_i - f_i}{s_i} + 1
    \end{equation}
    として計算される。
    ここで$\nu_i$が整数とならないときを許容しない場合と、切り捨て等の処理を行って処理を継続する場合がある。
    入力$x_{\gamma_1 \gamma_2 \cdots \gamma_n}$が与えられたとき、計算に用いられるのはパディングによって埋められた値も含まれる。
    よって、任意の$i$に対して$-p_i \le \gamma_i < \mu_i + p_i$として
    \begin{equation}
      x'_{\gamma_1 \gamma_2 \cdots \gamma_n} = \begin{cases}
        x_{\gamma_1 \gamma_2 \cdots \gamma_n}, & \forall i, 0 \le \gamma_i < \mu_i \\
        v_{\gamma_1 \gamma_2 \cdots \gamma_n}, & \text{otherwise}
      \end{cases}
    \end{equation}
    と書かれる$x'_{\gamma_1 \gamma_2 \cdots \gamma_n}$を用いて計算されるとする。
    ここで$v$はパディングによって埋められる値である。
    入力$x_{\gamma_1 \gamma_2 \cdots \gamma_n}$とフィルタ$w_{\alpha_1 \alpha_2 \cdots \alpha_n}$のよって出力$y_{\beta_1 \beta_2 \cdots \beta_n}$は
    \begin{equation}
      y_{\beta_1 \beta_2 \cdots \beta_n} \sum_{\alpha_1 \alpha_2 \cdots \alpha_n} x'_{\gamma_1 \gamma_2 \cdots \gamma_n} w_{\alpha_1 \alpha_2 \cdots \alpha_n} + b
    \end{equation}
    で計算され、各$\gamma_i$は$\alpha_i$と$\beta_i$によって
    \begin{equation}
      \gamma_i = s_i \beta_i + f_i - p_i - \alpha_i - 1
    \end{equation}
    と書かれる。
    ここで、$b$は各出力値に対するしきい値であり、畳み込み和後に全ての値に等しく加算される。

    畳み込み層の重みと入力による偏導関数は
    \begin{align}
      \partial_{x_{\gamma_1 \gamma_2 \cdots \gamma_n}} \func{f_{\beta_1 \beta_2 \cdots \beta_n}}{x, w} = \begin{cases}
        w_{\alpha_1 \alpha_2 \cdots \alpha_n}, & \forall i, \frac{\gamma_i - f_i + p_i + 1}{s_i} \le \beta_i < \frac{\gamma_i + p_i + 1}{s_i} \\
        0, & \text{otherwise}
      \end{cases} \\
      \partial_{w_{\alpha_1 \alpha_2 \cdots \alpha_n}} \func{f_{\beta_1 \beta_2 \cdots \beta_n}}{x, w} = \begin{cases}
        x'_{\gamma_1 \gamma_2 \cdots \gamma_n}, & \forall i, \frac{\alpha_i - f_i + 1}{s_i} \le \beta_i < \frac{\mu_i + \alpha_i - f_i + 2 p_i + 1}{s_i} \\
        0, & \text{otherwise}
      \end{cases} \\
      \partial_{b} \func{f_{\beta_1 \beta_2 \cdots \beta_n}}{x, w} = 1
    \end{align}
    となる。




\end{document}

